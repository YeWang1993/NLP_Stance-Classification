{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Set-up, Pre-processing and Hand-crafted Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Set-up\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=tf_config)\n",
    "\n",
    "set_session(sess)\n",
    "import gensim\n",
    "import keras\n",
    "import re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from tqdm import tqdm\n",
    "from sklearn import feature_extraction\n",
    "from pandas import DataFrame\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "np.random.seed(1003)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Input, Dense, Concatenate, Conv2D, Reshape, MaxPooling1D, Flatten, BatchNormalization, Activation, Dropout, Embedding\n",
    "import nltk\n",
    "_wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Specify the folder locations\n",
    "W2V_DIR = '/home/students/y2886wan/W/GoogleNews-vectors-negative300.bin'\n",
    "DATA_DIR = 'data/'\n",
    "#EMB_DIR = '/home/students/y2886wan/glove_data/glove.6B.50d.txt'\n",
    "\n",
    "# Hyperparameters \n",
    "MAX_SENT_LEN_1 = 16\n",
    "MAX_SENT_LEN_2 = 512\n",
    "LSTM_DIM = 16\n",
    "MAX_VOCAB_SIZE=26562\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 200\n",
    "\n",
    "\n",
    "# Loading data\n",
    "trainData_Heading = pd.read_csv(\"/home/students/y2886wan/Data/train_data.csv\")\n",
    "valData_Heading = pd.read_csv(\"/home/students/y2886wan/Data/validation_data.csv\")\n",
    "trainData_Bodies = pd.read_csv(\"/home/students/y2886wan/Data/article_body_texts.csv\")\n",
    "\n",
    "# Stop word\n",
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "\n",
    "# Data generater\n",
    "def generate_data(Heading,Bodies):\n",
    "\n",
    "    Data = pd.merge(Heading, Bodies, how='left', on='Body ID')\n",
    "\n",
    "    Data = Data.sample(frac=1, random_state=10) # Shuffle the rows\n",
    "    Data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    Data['Headline'] = Data['Headline'].apply(lambda x: str(x))\n",
    "    Data['articleBody'] = Data['articleBody'].apply(lambda x: str(x))\n",
    "    \n",
    "    return Data\n",
    "\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "# Cleaning data\n",
    "def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" U S \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "\n",
    "# Processing headline and bodies\n",
    "def process_questions(question_list, questions, question_list_name, dataframe):\n",
    "    for question in questions:\n",
    "        question_list.append(text_to_wordlist(question))\n",
    "        if len(question_list) % 100000 == 0:\n",
    "            progress = len(question_list)/len(dataframe) * 100\n",
    "            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))\n",
    "\n",
    "\n",
    "            \n",
    "# Overlap features extraction\n",
    "def get_pairwise_word_to_doc_freq(headlines, bodies):\n",
    "    \"\"\"\n",
    "    Get pairwise word to document frequency.\n",
    "    For index i, if sentence i in headlines and sentence i in bodies both\n",
    "    container word w, then w is counted only once.\n",
    "    Returns a dictionary mapping words to number of sentence pairs the word appears in.\n",
    "    \"\"\"\n",
    "    word_to_doc_cnt = defaultdict(int)\n",
    "\n",
    "    for s1, s2 in zip(headlines, bodies):\n",
    "        unique_tokens = set(s1) | set(s2)\n",
    "        for t in unique_tokens:\n",
    "            word_to_doc_cnt[t] += 1\n",
    "\n",
    "    return word_to_doc_cnt\n",
    "\n",
    "def get_pairwise_overlap_features(headlines, bodies, word_to_doc_cnt):\n",
    "    \"\"\"\n",
    "    Get overlap, idf weighted overlap, overlap excluding stopwords, and idf weighted overlap excluding stopwords.\n",
    "    \"\"\"\n",
    "    stoplist = set(stop_words)\n",
    "    num_docs = len(headlines)\n",
    "    overlap_feats = []\n",
    "\n",
    "    for s1, s2 in zip(headlines, bodies):\n",
    "        tokens_a_set, tokens_b_set = set(s1), set(s2)\n",
    "        intersect = tokens_a_set & tokens_b_set\n",
    "        overlap = len(intersect) / len(tokens_a_set) \n",
    "        idf_intersect = sum(np.math.log(num_docs / word_to_doc_cnt[w]) for w in intersect)\n",
    "        idf_weighted_overlap = idf_intersect / (len(tokens_a_set) + len(tokens_b_set))\n",
    "\n",
    "        tokens_a_set_no_stop = set(w for w in s1 if w not in stoplist)\n",
    "        tokens_b_set_no_stop = set(w for w in s2 if w not in stoplist)\n",
    "        intersect_no_stop = tokens_a_set_no_stop & tokens_b_set_no_stop\n",
    "        overlap_no_stop = len(intersect_no_stop) / (len(tokens_a_set_no_stop) + len(tokens_b_set_no_stop))\n",
    "        idf_intersect_no_stop = sum(np.math.log(num_docs / word_to_doc_cnt[w]) for w in intersect_no_stop)\n",
    "        idf_weighted_overlap_no_stop = idf_intersect_no_stop / (len(tokens_a_set_no_stop) + len(tokens_b_set_no_stop))\n",
    "        overlap_feats.append([overlap, idf_weighted_overlap, overlap_no_stop, idf_weighted_overlap_no_stop])\n",
    "\n",
    "    return overlap_feats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Refuting features extraction\n",
    "def refuting_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        # 'refute',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        features = [1 if word in clean_headline else 0 for word in _refuting_words]\n",
    "        features =list(features)\n",
    "        X.append(features)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "# Polarity features extraction\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "\n",
    "def polarity_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "\n",
    "    def calculate_polarity(text):\n",
    "        tokens = get_tokenized_lemmas(text)\n",
    "        return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        features = []\n",
    "        features.append(calculate_polarity(clean_headline))\n",
    "        features.append(calculate_polarity(clean_body))\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "# Concatenate training data and validation data\n",
    "test_count = len(valData_Heading)\n",
    "trainData = generate_data(trainData_Heading,trainData_Bodies)\n",
    "valData = generate_data(valData_Heading,trainData_Bodies)\n",
    "frames=[trainData,valData]\n",
    "trainData = pd.concat(frames)\n",
    "\n",
    "\n",
    "# Processing headline and bodies\n",
    "train_headling = []\n",
    "process_questions(train_headling, trainData.Headline, 'train_headling', trainData)\n",
    "train_bodies = []\n",
    "process_questions(train_bodies, trainData.articleBody, 'train_bodies', trainData)\n",
    "\n",
    "# Generating hand-crafted features\n",
    "cnt = get_pairwise_word_to_doc_freq(train_headling, train_bodies)\n",
    "overlap_features = get_pairwise_overlap_features(train_headling, train_bodies, cnt)\n",
    "refuting_feature = refuting_features(train_headling, train_bodies)\n",
    "polarity_feature = polarity_features(train_headling, train_bodies)\n",
    "\n",
    "word_seq = train_headling + train_bodies\n",
    "word_seq = [' '.join(word_tokenize(q)[:MAX_SENT_LEN_2]) for q in word_seq]\n",
    "\n",
    "\n",
    "# Tokenizer, transform words into interger identifier, create a dictionary\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(word_seq)\n",
    "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))\n",
    "\n",
    "\n",
    "# Transform all document into vectors using dictionary\n",
    "X = tokenizer.texts_to_sequences(word_seq)\n",
    "X = pad_sequences(X, maxlen=MAX_SENT_LEN_2, padding='post', truncating='post')\n",
    "\n",
    "# Separate headline and bodies\n",
    "X_q1 = X[:len(X)//2]\n",
    "X_q2 = X[len(X)//2:]\n",
    "del X\n",
    "\n",
    "# Reducing dimension of headline from 512 to 16\n",
    "X_q1 = np.delete(X_q1, range(MAX_SENT_LEN_1, MAX_SENT_LEN_2), axis = 1)\n",
    "\n",
    "# Transform each stance into vector\n",
    "Stance_list = [' '.join(word_tokenize(x)[:MAX_SENT_LEN_2]) for x in trainData['Stance']]\n",
    "wordstance=[]\n",
    "for i in Stance_list:\n",
    "    if i=='unrelated':\n",
    "        wordstance.append([0,0,0,1])\n",
    "    if i=='discuss':\n",
    "        wordstance.append([0,0,1,0])\n",
    "    if i=='agree':\n",
    "        wordstance.append([0,1,0,0])\n",
    "    if i=='disagree':\n",
    "        wordstance.append([1,0,0,0])\n",
    "\n",
    "\n",
    "# unpacked = [x[0] for x in y_stance]\n",
    "y = np.array(wordstance)\n",
    "\n",
    "# # Split Train and Test Set \n",
    "\n",
    "X_train_q1, X_test_q1, X_train_q2, X_test_q2, overlap_train, overlap_test, refuting_train, refuting_test, polarity_train, polarity_test, y_train, y_test = train_test_split(X_q1, X_q2, overlap_features, refuting_feature, polarity_feature, y, random_state=10, test_size=test_count)\n",
    "\n",
    "# Transform array to list\n",
    "polarity_train = [i.tolist() for i in polarity_train]\n",
    "polarity_test = [i.tolist() for i in polarity_test]\n",
    "\n",
    "# Concatenate features for train\n",
    "features_train = [[x+y+z] for x,y,z in zip(overlap_train, refuting_train, polarity_train)]\n",
    "feature_train = [x[0] for x in features_train]\n",
    "\n",
    "# Concatenate features for test\n",
    "features_test = [[x+y+z] for x,y,z in zip(overlap_test, refuting_test, polarity_test)]\n",
    "feature_test = [x[0] for x in features_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(W2V_DIR, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe embedding\n",
    "'''\n",
    "# Load GloVe word embeddings\n",
    "# Download Link: https://nlp.stanford.edu/projects/glove/\n",
    "print(\"[INFO]: Reading Word Embeddings ...\")\n",
    "# Data path\n",
    "embeddings = {}\n",
    "f = open(EMB_DIR)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = vector\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words embedding\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index) + 1, EMBEDDING_DIM))  # +1 is because the matrix indices start with 0\n",
    "for word, i in tokenizer.word_index.items():  # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "input_1 = Input(shape=(MAX_SENT_LEN_1, ), name='q1_input') #shape: a vector\n",
    "# Common embedding lookup layer\n",
    "emb_look_up = Embedding(input_dim=MAX_VOCAB_SIZE,\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        weights = [embeddings_matrix], \n",
    "                        trainable=False, \n",
    "                        mask_zero=False,\n",
    "                        name='q_embedding_lookup')\n",
    "\n",
    "emb_1 = emb_look_up(input_1)\n",
    "\n",
    "LSTM_Layer_1 = LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer_1')\n",
    "LSTM_Out = LSTM_Layer_1(emb_1)\n",
    "Dense_layer_1 = Dense(8, activation='sigmoid', name='output_layer_1')\n",
    "Dense_1_out = Dense_layer_1(LSTM_Out)\n",
    "Dense_layer_2 = Dense(4, activation='sigmoid', name='output_layer_2')\n",
    "Dense_2_out = Dense_layer_2(Dense_1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "input_2 = Input(shape=(MAX_SENT_LEN_2, ), name='q2_input')\n",
    "\n",
    "emb_2 = emb_look_up(input_2)\n",
    "\n",
    "LSTM_Layer_2 = LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer_2')\n",
    "LSTM_Out_2 = LSTM_Layer_2(emb_2)\n",
    "Dense_1 = Dense(256, activation='sigmoid', name='output_layer_3')\n",
    "Dense_Out_1 = Dense_1(LSTM_Out_2)\n",
    "Dense_2 = Dense(128, activation='sigmoid', name='output_layer_4')\n",
    "Dense_Out_2 = Dense_2(Dense_Out_1)\n",
    "Dense_3 = Dense(64, activation='sigmoid', name='output_layer_5')\n",
    "Dense_Out_3 = Dense_3(Dense_Out_2)\n",
    "Dense_4 = Dense(32, activation='sigmoid', name='output_layer_6')\n",
    "Dense_Out_4 = Dense_4(Dense_Out_3)\n",
    "Dense_5 = Dense(16, activation='sigmoid', name='output_layer_7')\n",
    "Dense_Out_5 = Dense_5(Dense_Out_4)\n",
    "Dense_6 = Dense(8, activation='sigmoid', name='output_layer_8')\n",
    "Dense_Out_6 = Dense_6(Dense_Out_5)\n",
    "Dense_7 = Dense(4, activation='sigmoid', name='output_layer_9')\n",
    "Dense_Out_7 = Dense_7(Dense_Out_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With batch-normalization, the output of a previous layer is mu-sigma normalized, \n",
    "# before it is fed into the next layer. \n",
    "# For feed-forward networks, batch-normalization is carried out \n",
    "\n",
    "# Additional features input\n",
    "input_3 = Input(shape=(21, ), name='feature_input')\n",
    "\n",
    "# Merge headline, bodies and hand-crafted features\n",
    "merged = Concatenate(name='headling_bodies_features_concat')([Dense_2_out, Dense_Out_7, input_3])\n",
    "\n",
    "# Dense layers\n",
    "dense_1 = Dense(units=10, name='headling_bodies_features_dense')(merged)\n",
    "bn_1 = BatchNormalization(name='batchnorm')(dense_1)\n",
    "relu_1 = Activation(activation='relu', name='relu_activation')(bn_1)\n",
    "dense_1_dropout = Dropout(0.2, name='dense_dropout')(relu_1)\n",
    "\n",
    "output_prob = Dense(units=4, activation='softmax', name='output_layer')(dense_1_dropout)\n",
    "\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=output_prob, name='text_pair_cnn')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "\n",
    "from keras import metrics\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training LSTM and CNN model\n",
    "model.fit(x = [X_train_q1, X_train_q2, np.array(feature_train)], \n",
    "          y = y_train, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=N_EPOCHS, \n",
    "          validation_data=([X_test_q1, X_test_q2, np.array(feature_test)], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_try_128_200epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict([X_test_q1, X_test_q2, np.array(feature_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = {0: 0, 1:0, 2: 0, 3: 0}\n",
    "pred_count = {0: 0, 1:0, 2: 0, 3: 0}\n",
    "true_count = {0: 0, 1:0, 2: 0, 3: 0}\n",
    "prediction2 = np.argmax(prediction, axis=-1)\n",
    "y_train2 = np.argmax(y_test, axis=-1)\n",
    "for p, t in zip(prediction2, y_train2):\n",
    "    if p == t:\n",
    "        correct[p] += 1\n",
    "    pred_count[p] += 1\n",
    "    true_count[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in correct:\n",
    "    c, t, p = correct[k], true_count[k], pred_count[k]\n",
    "    pre = c / p\n",
    "    rec = c / t\n",
    "    f1 = 2 * pre * rec / (pre + rec)\n",
    "    print(\"precision at {}: {}\".format(k, pre))\n",
    "    print(\"recall at {}: {}\".format(k, rec))\n",
    "    print(\"f1 at {}: {}\".format(k, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input test data and transform it into dictionary\n",
    "testData_Heading = pd.read_csv(\"/home/students/y2886wan/Data/test_data.csv\")\n",
    "testData = pd.merge(testData_Heading, trainData_Bodies, how='left', on='Body ID')\n",
    "testData['Headline'] = testData['Headline'].apply(lambda x: str(x))\n",
    "testData['articleBody'] = testData['articleBody'].apply(lambda x: str(x))\n",
    "\n",
    "# Generating the features for test data \n",
    "test_headling = []\n",
    "process_questions(test_headling, testData.Headline, 'test_headling', testData)\n",
    "test_bodies = []\n",
    "process_questions(test_bodies, testData.articleBody, 'testData_bodies', testData)\n",
    "\n",
    "### overlap\n",
    "cnt_t = get_pairwise_word_to_doc_freq(test_headling, test_bodies)\n",
    "overlap_features_t = get_pairwise_overlap_features(test_headling, test_bodies, cnt_t)\n",
    "\n",
    "### refuting\n",
    "refuting_feature_t = refuting_features(test_headling, test_bodies)\n",
    "\n",
    "### polarity\n",
    "polarity_feature_t = polarity_features(test_headling, test_bodies)\n",
    "polarity_t = [i.tolist() for i in polarity_feature_t]\n",
    "\n",
    "\n",
    "features_t = [[x+y+z] for x,y,z in zip(overlap_features_t, refuting_feature_t, polarity_t)]\n",
    "feature_t = [x[0] for x in features_t]\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "word_seq_t = test_headling + test_bodies\n",
    "word_seq_t = [' '.join(word_tokenize(q)[:MAX_SENT_LEN_2]) for q in word_seq_t]\n",
    "tokenizer_t = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer_t.fit_on_texts(word_seq_t)\n",
    "\n",
    "# Transform test data into matrix using dictionary\n",
    "XT = tokenizer_t.texts_to_sequences(word_seq_t)\n",
    "XT = pad_sequences(XT, maxlen=MAX_SENT_LEN_2, padding='post', truncating='post')\n",
    "\n",
    "X_q1_t = XT[:len(XT)//2]\n",
    "X_q2_t = XT[len(XT)//2:]\n",
    "X_q1_t = np.delete(X_q1_t, range(MAX_SENT_LEN_1, MAX_SENT_LEN_2), axis = 1)\n",
    "del XT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "prediction2 = model.predict(x = [X_q1_t, X_q2_t, np.array(feature_t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the highest probability position of each vector\n",
    "predicted = [[np.argmax(x)] for x in prediction2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all position into string (four classes) \n",
    "wordstance=[]\n",
    "for i in predicted:\n",
    "    if i==[0]:\n",
    "        wordstance.append(\"disagree\")\n",
    "    if i==[1]:\n",
    "        wordstance.append(\"agree\")\n",
    "    if i==[2]:\n",
    "        wordstance.append(\"discuss\")\n",
    "    if i==[3]:\n",
    "        wordstance.append(\"unrelated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction save as CSV file\n",
    "wordseries=pd.Series(wordstance) # all stance\n",
    "testData = pd.read_csv(\"/home/students/y2886wan/Data/test_data.csv\")\n",
    "testData['Stance'] = wordseries.values\n",
    "results = testData.to_csv('answer.csv',index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
